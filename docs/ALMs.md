

# Augmented Large Language Models: A Comprehensive SOTA Treatment

---

## Table of Contents
1. [Augmented Large Language Models â€” Formal Framework](#1)
2. [Retrieval-Augmented Generation (RAG)](#2)
3. [RAG Evaluation](#3)
4. [Tool Calling with LLMs](#4)
5. [LLM Augmentation with Agents](#5)

---

# 1. Augmented Large Language Models (ALMs)

## 1.1 Definition

An **Augmented Large Language Model** is a system $\mathcal{S}$ that extends a frozen or fine-tuned parametric language model $\mathcal{M}_\theta$ by coupling it with one or more **non-parametric** or **external functional modules** $\{E_1, E_2, \dots, E_k\}$, such that the composite system's generative distribution surpasses what $\mathcal{M}_\theta$ can express alone.

Formally, whereas a vanilla LLM models:

$$P_\theta(y \mid x) = \prod_{t=1}^{T} P_\theta(y_t \mid y_{<t}, x)$$

an Augmented LLM models:

$$P_{\mathcal{S}}(y \mid x) = \prod_{t=1}^{T} P_\theta\!\Big(y_t \;\Big|\; y_{<t},\; x,\; \bigoplus_{i=1}^{k} E_i\big(\phi_i(x, y_{<t})\big)\Big)$$

where:
- $\phi_i(\cdot)$ is a **query formulation function** that constructs a request to external module $E_i$
- $\bigoplus$ denotes an **aggregation operator** (concatenation, cross-attention fusion, interleaving, or gated mixture)
- $E_i$ can be a retriever, tool API, code interpreter, memory buffer, or another model

## 1.2 Motivation â€” Why Augmentation is Necessary

Standard LLMs suffer from well-characterized failure modes that are **inherent to purely parametric architectures**:

| Failure Mode | Formal Characterization |
|---|---|
| **Knowledge Cutoff** | $P_\theta(y \mid x)$ is conditioned only on training corpus $\mathcal{D}_{\text{train}}$ with temporal bound $t_{\text{max}}$ |
| **Hallucination** | Model assigns $P_\theta(y \mid x) > \epsilon$ to factually incorrect $y$ due to distributional memorization artifacts |
| **Computation Bottleneck** | All reasoning must occur within fixed forward-pass depth $L$ and hidden dimension $d$; no external symbolic computation |
| **Static Knowledge** | Parameters $\theta$ encode a snapshot; updating requires retraining with cost $\mathcal{O}(|\theta| \cdot |\mathcal{D}|)$ |
| **Groundedness Gap** | No mechanism to verify generated claims against authoritative sources at inference time |

## 1.3 Taxonomy of Augmentation Modalities

```
                    Augmented LLM
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚                  â”‚
    Retrieval        Tool Use           Agentic
   Augmentation     Augmentation      Augmentation
          â”‚              â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
   â”‚             â”‚  â”‚        â”‚     â”‚              â”‚
 Sparse       Dense  API   Code   Single-Agent  Multi-Agent
 (BM25)     (DPR)  Calls  Exec   (ReAct)       (Debate)
```

## 1.4 Formal Augmentation Categories

**Category 1 â€” Retrieval Augmentation ($E_{\text{ret}}$):**

$$E_{\text{ret}}(q) = \text{Top-}k\Big(\{d_j\}_{j=1}^{N},\; \text{sim}(f_q(q),\; f_d(d_j))\Big)$$

**Category 2 â€” Tool Augmentation ($E_{\text{tool}}$):**

$$E_{\text{tool}}(a) = \texttt{Execute}\big(\text{API}_a,\; \text{args}(a)\big) \rightarrow r_a$$

**Category 3 â€” Agentic Augmentation ($E_{\text{agent}}$):**

$$E_{\text{agent}}(x) = \text{ControlLoop}\big(\mathcal{M}_\theta,\; \{E_1, \dots, E_k\},\; \text{objective}(x)\big)$$

## 1.5 The Augmentation Integration Equation

The key architectural decision is **where** and **how** external information integrates into the generation process. We define the **Augmented Context Window**:

$$C_{\text{aug}}(x, t) = \Big[x \;\|\; \underbrace{E_{\text{ret}}(\phi_{\text{ret}}(x, y_{<t}))}_{\text{retrieved documents}} \;\|\; \underbrace{E_{\text{tool}}(\phi_{\text{tool}}(x, y_{<t}))}_{\text{tool outputs}} \;\|\; \underbrace{M_{<t}}_{\text{memory trace}}\Big]$$

where $\|$ denotes sequence concatenation and $M_{<t}$ represents accumulated agent memory.

The generation at each step then becomes:

$$y_t \sim P_\theta\big(y_t \mid C_{\text{aug}}(x, t)\big)$$

---

# 2. Retrieval-Augmented Generation (RAG)

## 2.1 Definition

**Retrieval-Augmented Generation (RAG)** is a framework where a language model's generation is conditioned not only on the input query $q$ but also on a set of documents $\mathcal{D}_q$ dynamically retrieved from an external knowledge corpus $\mathcal{K} = \{d_1, d_2, \dots, d_N\}$ at inference time.

$$P_{\text{RAG}}(y \mid q) = \sum_{d \in \mathcal{K}} P_{\text{ret}}(d \mid q) \cdot P_\theta(y \mid q, d)$$

In the practical **top-$k$ marginalization** variant:

$$P_{\text{RAG}}(y \mid q) \approx \sum_{d \in \text{Top-}k(\mathcal{K}, q)} \frac{\exp\big(\text{sim}(q, d)/\tau\big)}{\sum_{d' \in \text{Top-}k} \exp\big(\text{sim}(q, d')/\tau\big)} \cdot P_\theta(y \mid q, d)$$

where $\tau$ is a temperature parameter controlling retrieval distribution sharpness.

## 2.2 RAG Architecture â€” End-to-End Pipeline

![Image description](assets/rag_system_architecture_1772170219324.png)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG SYSTEM ARCHITECTURE                         â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Query q  â”‚â”€â”€â”€â–¶â”‚  Query     â”‚â”€â”€â”€â–¶â”‚ Retriever â”‚â”€â”€â”€â–¶â”‚ Re-Ranker â”‚  â”‚
â”‚  â”‚          â”‚    â”‚  Encoder   â”‚    â”‚  (ANN)    â”‚    â”‚ (Cross-   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  f_q(Â·)   â”‚    â”‚           â”‚    â”‚  Encoder) â”‚  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚              Knowledge Corpus  K                      â”‚ â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”                 â”‚ â”‚        â”‚
â”‚  â”‚  â”‚ dâ‚ â”‚ â”‚ dâ‚‚ â”‚ â”‚ dâ‚ƒ â”‚  ...   â”‚ dâ‚™ â”‚                 â”‚ â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”˜                 â”‚ â”‚        â”‚
â”‚  â”‚         â”‚                                            â”‚ â”‚        â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚ â”‚        â”‚
â”‚  â”‚    â”‚ Document Encoder f_d  â”‚                         â”‚ â”‚        â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚ â”‚        â”‚
â”‚  â”‚         â”‚                                            â”‚ â”‚        â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚ â”‚        â”‚
â”‚  â”‚    â”‚ Vector Index (FAISS/  â”‚                         â”‚ â”‚        â”‚
â”‚  â”‚    â”‚ HNSW / ScaNN)        â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                           â”‚        â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                           â”‚      Context Assembly                â”‚ â”‚
â”‚                           â”‚  C = [Instruction âˆ¥ dâ‚ âˆ¥...âˆ¥ dâ‚– âˆ¥ q]â”‚ â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                          â”‚                         â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                           â”‚      Generator  M_Î¸                  â”‚ â”‚
â”‚                           â”‚      P_Î¸(y | C)                      â”‚ â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                          â”‚                         â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                           â”‚    Response  y        â”‚                 â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.3 Component-Level Deep Dive

### 2.3.1 Indexing Phase

**Definition:** The offline process of converting raw documents into searchable vector representations stored in an approximate nearest neighbor (ANN) index.

**Chunking Strategy:** Given a document $D$ of length $|D|$ tokens, we partition it into chunks:

$$D \rightarrow \{c_1, c_2, \dots, c_m\} \quad \text{where} \quad c_i = D[s_i : s_i + w], \quad s_{i+1} = s_i + w - o$$

- $w$ = chunk window size (tokens)
- $o$ = overlap size (tokens)
- $m = \lceil (|D| - o) / (w - o) \rceil$

**Embedding:** Each chunk is encoded via a bi-encoder:

$$\mathbf{v}_i = f_d(c_i) \in \mathbb{R}^{d_{\text{emb}}}$$

typically normalized: $\hat{\mathbf{v}}_i = \mathbf{v}_i / \|\mathbf{v}_i\|_2$

**Index Construction:** Vectors are inserted into an ANN structure (HNSW, IVF-PQ, etc.) that supports sublinear search:

$$\text{Index}: \{\hat{\mathbf{v}}_1, \dots, \hat{\mathbf{v}}_N\} \rightarrow \mathcal{I} \quad \text{s.t.} \quad \text{Search}(\mathcal{I}, \mathbf{q}, k) \in \mathcal{O}(\log N)$$

---

### 2.3.2 Retrieval Phase

**Bi-Encoder Retrieval (Dense):**

$$\text{sim}(q, d_i) = \langle f_q(q),\; f_d(d_i) \rangle = \mathbf{q}^\top \mathbf{d}_i$$

where $f_q$ and $f_d$ may share parameters (single-encoder) or be separate (dual-encoder).

**Sparse Retrieval (BM25):**

$$\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot \big(1 - b + b \cdot \frac{|d|}{\text{avgdl}}\big)}$$

where $f(t, d)$ is term frequency, $k_1$ and $b$ are hyperparameters.

**Hybrid Retrieval:** A convex combination:

$$\text{score}_{\text{hybrid}}(q, d) = \alpha \cdot \text{sim}_{\text{dense}}(q, d) + (1 - \alpha) \cdot \text{BM25}_{\text{norm}}(q, d)$$

where $\alpha \in [0, 1]$ controls the interpolation.

---

### 2.3.3 Re-Ranking Phase

A cross-encoder $\text{CE}(q, d)$ jointly attends over query-document pairs:

$$\text{CE}(q, d) = \sigma\big(W_r \cdot \text{CLS}\big(\text{Transformer}([q; \texttt{[SEP]}; d])\big) + b_r\big)$$

This is computationally expensive ($\mathcal{O}(k \cdot (|q| + |d|)^2)$) but provides superior relevance estimation due to full cross-attention.

**Re-rank scoring with Listwise loss (ListMLE):**

$$\mathcal{L}_{\text{ListMLE}} = -\sum_{i=1}^{k} \log \frac{\exp\big(s_{\pi(i)}\big)}{\sum_{j=i}^{k} \exp\big(s_{\pi(j)}\big)}$$

where $\pi$ is the ground-truth permutation and $s_i = \text{CE}(q, d_i)$.

---

### 2.3.4 Generation Phase (Reader)

The generator conditions on the augmented context:

$$P_\theta(y \mid q, \mathcal{D}_q) = \prod_{t=1}^{T} P_\theta\!\big(y_t \mid y_{<t}, [q \;\|\; d_1 \;\|\; \dots \;\|\; d_k]\big)$$

**Fusion-in-Decoder (FiD)** variant encodes each document independently, then cross-attends:

$$\mathbf{H}_i = \text{Encoder}([q \;\|\; d_i]) \quad \forall i \in \{1, \dots, k\}$$

$$\mathbf{H}_{\text{fused}} = [\mathbf{H}_1; \mathbf{H}_2; \dots; \mathbf{H}_k]$$

$$y_t = \text{Decoder}(y_{<t}, \mathbf{H}_{\text{fused}})$$

This scales linearly $\mathcal{O}(k \cdot |d|^2 + T \cdot k \cdot |d|)$ rather than quadratically in total context length.

## 2.4 RAG Variants Taxonomy

| Variant | Retrieval Timing | Key Mechanism |
|---|---|---|
| **Naive RAG** | Once, before generation | Single retrieval â†’ generate |
| **Advanced RAG** | Once, with pre/post-retrieval processing | Query rewriting + re-ranking |
| **Modular RAG** | Multiple times, adaptively | Conditional retrieval triggers |
| **Self-RAG** | Per-segment, with self-reflection | Model decides when to retrieve and critiques retrieved content |
| **CRAG (Corrective RAG)** | With retrieval quality assessment | Evaluates retrieval relevance; falls back to web search |
| **Adaptive RAG** | Query-complexity-dependent | Classifier routes to no-retrieval, single-retrieval, or iterative |

### 2.4.1 Self-RAG â€” Formal Definition

Self-RAG introduces **reflection tokens** $\{r_{\text{ret}}, r_{\text{rel}}, r_{\text{sup}}, r_{\text{use}}\}$:

$$P_{\text{Self-RAG}}(y, \mathbf{r} \mid q) = \prod_{t} P_\theta(r_{\text{ret}}^{(t)} \mid q, y_{<t}) \cdot \begin{cases} P_\theta(d^{(t)} \mid q) \cdot P_\theta(y_t, r_{\text{rel}}^{(t)}, r_{\text{sup}}^{(t)} \mid q, d^{(t)}, y_{<t}) & \text{if } r_{\text{ret}}^{(t)} = \texttt{yes} \\ P_\theta(y_t \mid q, y_{<t}) & \text{if } r_{\text{ret}}^{(t)} = \texttt{no} \end{cases}$$

where:
- $r_{\text{ret}} \in \{\texttt{yes}, \texttt{no}\}$ â€” whether to retrieve
- $r_{\text{rel}} \in \{\texttt{relevant}, \texttt{irrelevant}\}$ â€” document relevance
- $r_{\text{sup}} \in \{\texttt{fully}, \texttt{partially}, \texttt{none}\}$ â€” response support level
- $r_{\text{use}} \in \{1, 2, 3, 4, 5\}$ â€” overall utility score

## 2.5 Pseudo-Algorithms

### Algorithm 1: RAG Indexing Pipeline

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: RAG-INDEX
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    K = {Dâ‚, Dâ‚‚, ..., Dâ‚™}          â–· Raw document corpus
    f_d(Â·)                           â–· Document encoder model
    w                                â–· Chunk window size (tokens)
    o                                â–· Chunk overlap size (tokens)
    ANN_config                       â–· Index hyperparameters (type, nprobe, efConstruction)

OUTPUT:
    I                                â–· Searchable vector index
    ChunkStore                       â–· Mapping: chunk_id â†’ chunk_text + metadata

PROCEDURE:
    1.  ChunkStore â† âˆ…
    2.  VectorBuffer â† âˆ…
    3.  global_id â† 0

    4.  FOR each document Dâ±¼ âˆˆ K DO:
    5.      chunks â† SEGMENT(Dâ±¼, w, o)
                â–· SEGMENT splits Dâ±¼ into overlapping windows
                â–· cáµ¢ = Dâ±¼[sáµ¢ : sáµ¢ + w], sáµ¢â‚Šâ‚ = sáµ¢ + (w âˆ’ o)

    6.      FOR each chunk cáµ¢ âˆˆ chunks DO:
    7.          váµ¢ â† f_d(cáµ¢)                    â–· Encode to dense vector âˆˆ â„^d
    8.          vÌ‚áµ¢ â† váµ¢ / â€–váµ¢â€–â‚‚                â–· L2-normalize
    9.          ChunkStore[global_id] â† (cáµ¢, metadata(Dâ±¼, i))
   10.          VectorBuffer â† VectorBuffer âˆª {(global_id, vÌ‚áµ¢)}
   11.          global_id â† global_id + 1
   12.      END FOR
   13.  END FOR

   14.  I â† BUILD_ANN_INDEX(VectorBuffer, ANN_config)
            â–· Constructs HNSW / IVF-PQ / ScaNN index

   15.  RETURN (I, ChunkStore)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm 2: RAG Inference Pipeline

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: RAG-INFERENCE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· User query (natural language)
    f_q(Â·)                           â–· Query encoder
    I                                â–· Pre-built vector index
    ChunkStore                       â–· Chunk text mapping
    M_Î¸                              â–· Generator LLM
    k                                â–· Number of documents to retrieve
    CE(Â·,Â·)                          â–· Cross-encoder re-ranker (optional)
    k_rerank                         â–· Number of documents after re-ranking

OUTPUT:
    y                                â–· Generated response
    Sources                          â–· Attribution source list

PROCEDURE:
    â”€â”€â”€â”€ STAGE 1: QUERY PROCESSING â”€â”€â”€â”€
    1.  q' â† QUERY_REWRITE(M_Î¸, q)
            â–· Optional: HyDE, multi-query expansion, step-back prompting
    2.  q_vec â† f_q(q') / â€–f_q(q')â€–â‚‚

    â”€â”€â”€â”€ STAGE 2: RETRIEVAL â”€â”€â”€â”€
    3.  CandidateIDs â† ANN_SEARCH(I, q_vec, k_initial)
            â–· k_initial â‰¥ k, retrieve broader candidate set
    4.  Candidates â† {(id, ChunkStore[id].text, sim(q_vec, I[id])) 
                       for id âˆˆ CandidateIDs}

    â”€â”€â”€â”€ STAGE 3: RE-RANKING â”€â”€â”€â”€
    5.  IF CE is provided THEN:
    6.      FOR each (id, text, _) âˆˆ Candidates DO:
    7.          score_id â† CE(q, text)
    8.      END FOR
    9.      RankedDocs â† TOP-k_rerank(Candidates, by score_id)
   10.  ELSE:
   11.      RankedDocs â† TOP-k(Candidates, by similarity)
   12.  END IF

    â”€â”€â”€â”€ STAGE 4: CONTEXT ASSEMBLY â”€â”€â”€â”€
   13.  Context â† ASSEMBLE([
            SystemPrompt,
            "Retrieved Documents:",
            FORMAT(RankedDocs),      â–· [1] doc_text_1 \n [2] doc_text_2 ...
            "Query:", q
        ])

    â”€â”€â”€â”€ STAGE 5: GENERATION â”€â”€â”€â”€
   14.  y â† M_Î¸.generate(Context, sampling_params)
   15.  Sources â† EXTRACT_ATTRIBUTIONS(y, RankedDocs)

   16.  RETURN (y, Sources)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm 3: Self-RAG Inference

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: SELF-RAG-INFERENCE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· User query
    M_Î¸                              â–· Self-RAG trained model (generates reflection tokens)
    RETRIEVER(Â·, k)                  â–· Retrieval function
    T_segments                       â–· Max number of generation segments

OUTPUT:
    y                                â–· Final response with self-assessed quality

PROCEDURE:
    1.  y â† ""
    2.  FOR t = 1 TO T_segments DO:

    3.      r_ret â† M_Î¸.predict_token(
                type=RETRIEVE_DECISION, 
                context=(q, y)
            )       â–· Output âˆˆ {yes, no, continue}

    4.      IF r_ret = "yes" THEN:
    5.          D_t â† RETRIEVER(FORMULATE_QUERY(q, y), k)
    6.          CandidateOutputs â† âˆ…

    7.          FOR each d âˆˆ D_t DO:
    8.              r_rel â† M_Î¸.predict_token(
                        type=RELEVANCE, 
                        context=(q, d)
                    )       â–· âˆˆ {relevant, irrelevant}
    9.              IF r_rel = "irrelevant" THEN CONTINUE

   10.              y_seg â† M_Î¸.generate_segment(q, d, y)
   11.              r_sup â† M_Î¸.predict_token(
                        type=SUPPORT, 
                        context=(q, d, y_seg)
                    )       â–· âˆˆ {fully_supported, partially, no_support}
   12.              r_use â† M_Î¸.predict_token(
                        type=UTILITY, 
                        context=(q, y âˆ¥ y_seg)
                    )       â–· âˆˆ {1, 2, 3, 4, 5}

   13.              score â† w_relÂ·ğŸ™[r_rel=rel] + w_supÂ·SCORE(r_sup) + w_useÂ·r_use
   14.              CandidateOutputs â† CandidateOutputs âˆª {(y_seg, d, score)}
   15.          END FOR

   16.          (y_best, d_best, _) â† argmax(CandidateOutputs, by score)
   17.          y â† y âˆ¥ y_best

   18.      ELSE:      â–· No retrieval needed
   19.          y_seg â† M_Î¸.generate_segment(q, y)
   20.          y â† y âˆ¥ y_seg
   21.      END IF

   22.      IF M_Î¸.predicts_EOS(y) THEN BREAK
   23.  END FOR

   24.  RETURN y
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

# 3. RAG Evaluation

## 3.1 Definition

**RAG Evaluation** is the systematic, multi-dimensional assessment of a Retrieval-Augmented Generation system across its decomposed components (retriever, re-ranker, generator) and their emergent interactions, quantified through both **component-level** and **end-to-end** metrics that measure retrieval quality, generation faithfulness, answer correctness, and absence of hallucination.

The evaluation problem can be stated as: given a benchmark $\mathcal{B} = \{(q_i, y_i^*, \mathcal{D}_i^*)\}_{i=1}^{n}$ where $y_i^*$ is the gold answer and $\mathcal{D}_i^*$ is the set of gold-relevant documents, compute a vector of metrics:

$$\mathbf{m}(q_i) = \big[\underbrace{m_{\text{ret}}}_{\text{Retrieval}},\; \underbrace{m_{\text{gen}}}_{\text{Generation}},\; \underbrace{m_{\text{e2e}}}_{\text{End-to-End}}\big]$$

## 3.2 Evaluation Dimensions and Metrics

### 3.2.1 Retrieval Component Metrics

**Context Precision** â€” Measures the proportion of retrieved documents that are relevant, weighted by rank:

$$\text{ContextPrecision}@k = \frac{1}{\text{|relevant docs in top-}k|} \sum_{i=1}^{k} \Big(\text{Precision}@i \times \mathbb{1}[d_i \in \mathcal{D}^*]\Big)$$

**Context Recall** â€” Measures coverage of gold-relevant information:

$$\text{ContextRecall} = \frac{|\{s \in y^* : \exists\; d \in \mathcal{D}_{\text{retrieved}},\; s \text{ is attributable to } d\}|}{|\{s \in y^*\}|}$$

where $s$ represents individual claims/sentences in the gold answer.

**Mean Reciprocal Rank (MRR):**

$$\text{MRR} = \frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \frac{1}{\text{rank}_i}$$

where $\text{rank}_i$ is the position of the first relevant document for query $q_i$.

**Normalized Discounted Cumulative Gain (nDCG@k):**

$$\text{DCG}@k = \sum_{i=1}^{k} \frac{2^{\text{rel}_i} - 1}{\log_2(i + 1)}$$

$$\text{nDCG}@k = \frac{\text{DCG}@k}{\text{IDCG}@k}$$

where $\text{IDCG}@k$ is the ideal DCG with perfect ranking.

### 3.2.2 Generation Component Metrics

**Faithfulness** â€” Measures whether every claim in the generated answer is supported by retrieved context:

$$\text{Faithfulness}(y, \mathcal{D}_{\text{ret}}) = \frac{|\{c \in \text{CLAIMS}(y) : \text{ENTAILED}(c, \mathcal{D}_{\text{ret}})\}|}{|\text{CLAIMS}(y)|}$$

where $\text{CLAIMS}(y)$ decomposes $y$ into atomic factual claims.

**Answer Relevance** â€” Measures whether the answer addresses the query (independent of correctness):

$$\text{AnswerRelevance}(q, y) = \frac{1}{n} \sum_{i=1}^{n} \text{sim}\big(q,\; \hat{q}_i\big)$$

where $\hat{q}_i$ are synthetically generated questions from $y$ using a reverse-generation model, and $\text{sim}$ is cosine similarity in embedding space.

**Hallucination Rate:**

$$\text{HallucinationRate}(y, \mathcal{D}_{\text{ret}}) = 1 - \text{Faithfulness}(y, \mathcal{D}_{\text{ret}})$$

### 3.2.3 End-to-End Metrics

**Answer Correctness** â€” Combines semantic similarity and factual overlap:

$$\text{Correctness}(y, y^*) = \beta \cdot F_1(y, y^*) + (1 - \beta) \cdot \text{SemanticSim}(y, y^*)$$

where $F_1$ operates on extracted claim sets:

$$F_1 = \frac{2 \cdot |\text{TP}|}{2 \cdot |\text{TP}| + |\text{FP}| + |\text{FN}|}$$

- $\text{TP}$: claims in $y$ that are also in $y^*$
- $\text{FP}$: claims in $y$ not in $y^*$ (hallucinations)
- $\text{FN}$: claims in $y^*$ not in $y$ (missing information)

## 3.3 Evaluation Frameworks

### 3.3.1 RAGAS (Retrieval Augmented Generation Assessment)

RAGAS defines four core metrics computed **without gold labels** (reference-free):

| Metric | What It Measures | Computation Method |
|---|---|---|
| **Faithfulness** | Claims grounded in context | LLM-based claim extraction â†’ NLI verification |
| **Answer Relevance** | Response addresses query | Reverse question generation â†’ similarity |
| **Context Precision** | Relevant context ranked higher | LLM judges relevance per retrieved chunk |
| **Context Recall** | Gold answer sentences covered | LLM checks attribution of each gold claim |

**RAGAS Aggregate Score:**

$$\text{RAGAS}_{\text{score}} = \text{HarmonicMean}(\text{Faithfulness}, \text{AnswerRelevance}, \text{ContextPrecision}, \text{ContextRecall})$$

### 3.3.2 LLM-as-Judge Evaluation

An evaluator LLM $\mathcal{M}_{\text{judge}}$ scores along predefined rubrics:

$$s = \mathcal{M}_{\text{judge}}\big(\text{RUBRIC},\; q,\; y,\; \mathcal{D}_{\text{ret}},\; y^*\big) \in [1, 5]$$

**Calibration Check:** To validate judge reliability:

$$\kappa = \frac{P_o - P_e}{1 - P_e}$$

where $\kappa$ is Cohen's kappa between LLM-judge and human annotators, $P_o$ is observed agreement, $P_e$ is chance agreement.

## 3.4 Diagnostic Failure Analysis Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG FAILURE DIAGNOSIS MATRIX                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Retrieval   â”‚  Generation  â”‚  Diagnosis                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ Good      â”‚  âœ“ Good      â”‚  System working correctly         â”‚
â”‚  âœ“ Good      â”‚  âœ— Bad       â”‚  Generator failure: ignoring      â”‚
â”‚              â”‚              â”‚  context, instruction-following    â”‚
â”‚              â”‚              â”‚  deficit, or insufficient capacity â”‚
â”‚  âœ— Bad       â”‚  âœ“ Good      â”‚  Lucky generation from parametric â”‚
â”‚              â”‚              â”‚  knowledge (fragile; not reliable) â”‚
â”‚  âœ— Bad       â”‚  âœ— Bad       â”‚  Retrieval failure: fix indexing,  â”‚
â”‚              â”‚              â”‚  chunking, query formulation,      â”‚
â”‚              â”‚              â”‚  or embedding model first          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.5 Pseudo-Algorithm: Comprehensive RAG Evaluation

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: RAG-EVALUATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    B = {(qáµ¢, yáµ¢*, Dáµ¢*)}          â–· Evaluation benchmark
    RAG_SYSTEM(Â·)                   â–· System under test
    M_judge                         â–· LLM evaluator (for reference-free metrics)
    NLI_MODEL(Â·)                    â–· Natural language inference model

OUTPUT:
    MetricReport                    â–· Per-query and aggregate scores

PROCEDURE:
    1.  Results â† []

    2.  FOR each (q, y*, D*) âˆˆ B DO:

        â”€â”€â”€â”€ Run RAG System â”€â”€â”€â”€
    3.      (y, D_ret, metadata) â† RAG_SYSTEM(q)

        â”€â”€â”€â”€ Retrieval Metrics â”€â”€â”€â”€
    4.      ctx_precision â† CONTEXT_PRECISION(D_ret, D*, ranks)
    5.      ctx_recall â† CONTEXT_RECALL(y*, D_ret, M_judge)
    6.      mrr â† 1 / FIRST_RELEVANT_RANK(D_ret, D*)
    7.      ndcg â† NDCG(D_ret, D*, k)

        â”€â”€â”€â”€ Generation Metrics â”€â”€â”€â”€
    8.      claims â† EXTRACT_CLAIMS(y, M_judge)
                â–· Decompose y into atomic factual propositions

    9.      supported_count â† 0
   10.      FOR each claim c âˆˆ claims DO:
   11.          verdict â† NLI_MODEL(premise=CONCAT(D_ret), hypothesis=c)
   12.          IF verdict = ENTAILMENT THEN:
   13.              supported_count â† supported_count + 1
   14.      END FOR
   15.      faithfulness â† supported_count / |claims|

   16.      reverse_queries â† GENERATE_QUESTIONS(y, M_judge, n=3)
   17.      answer_relevance â† MEAN([sim(q, qÌ‚) for qÌ‚ âˆˆ reverse_queries])

        â”€â”€â”€â”€ End-to-End Metrics â”€â”€â”€â”€
   18.      claims_y â† EXTRACT_CLAIMS(y, M_judge)
   19.      claims_y_star â† EXTRACT_CLAIMS(y*, M_judge)
   20.      TP â† |claims_y âˆ© claims_y_star|    â–· via semantic matching
   21.      FP â† |claims_y \ claims_y_star|
   22.      FN â† |claims_y_star \ claims_y|
   23.      f1_correctness â† 2Â·TP / (2Â·TP + FP + FN)
   24.      semantic_sim â† COSINE(EMBED(y), EMBED(y*))
   25.      correctness â† Î²Â·f1_correctness + (1âˆ’Î²)Â·semantic_sim

        â”€â”€â”€â”€ Aggregate per query â”€â”€â”€â”€
   26.      Results.APPEND({
                ctx_precision, ctx_recall, mrr, ndcg,
                faithfulness, answer_relevance,
                correctness, hallucination_rate: 1âˆ’faithfulness
            })
   27.  END FOR

    â”€â”€â”€â”€ Compute Aggregate â”€â”€â”€â”€
   28.  MetricReport â† {
            metric_name: MEAN([r[metric_name] for r âˆˆ Results])
            for each metric_name
        }
   29.  MetricReport["ragas_score"] â† HARMONIC_MEAN(
            MetricReport["faithfulness"],
            MetricReport["answer_relevance"],
            MetricReport["ctx_precision"],
            MetricReport["ctx_recall"]
        )

   30.  RETURN MetricReport
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

# 4. Tool Calling with LLMs

## 4.1 Definition

**Tool Calling** (also termed **function calling** or **tool use**) is the capability of an LLM $\mathcal{M}_\theta$ to recognize when its parametric knowledge or computational abilities are insufficient for a given query, **emit a structured invocation** of an external function or API, receive the result, and integrate that result into its ongoing generation.

Formally, we augment the token vocabulary $\mathcal{V}$ with a set of **tool-call tokens** and define:

$$\mathcal{V}_{\text{aug}} = \mathcal{V} \cup \mathcal{V}_{\text{tool}}$$

where $\mathcal{V}_{\text{tool}}$ includes structured markers such as $\texttt{<tool\_call>}$, $\texttt{</tool\_call>}$, $\texttt{<tool\_result>}$, etc.

The generation process becomes a **hybrid autoregressive-reactive loop:**

$$y_t \sim \begin{cases} P_\theta(y_t \mid y_{<t}, x, R) & \text{if } y_{t-1} \notin \mathcal{V}_{\text{tool}} \\ \texttt{EXECUTE}(\text{parse\_tool\_call}(y_{<t})) \rightarrow R & \text{if } y_{t-1} = \texttt{</tool\_call>} \end{cases}$$

where $R$ is the tool result injected back into the context.

## 4.2 Tool Specification Schema

A tool is defined as a tuple:

$$\mathcal{T} = (\text{name}, \text{description}, \text{parameters}, \text{returns})$$

where $\text{parameters}$ follows a typed schema:

$$\text{parameters} = \{(p_i, \text{type}_i, \text{description}_i, \text{required}_i)\}_{i=1}^{m}$$

This is typically serialized as a JSON Schema object and provided in the system prompt or a dedicated tool-definition section.

## 4.3 Tool-Call Resolution Process

```
     User Query
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   M_Î¸        â”‚
  â”‚  (Planning)  â”‚â”€â”€â”€â”€ "Do I need a tool?" â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
         â”‚ NO                                   â”‚ YES
         â–¼                                      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Direct      â”‚                    â”‚  Emit structured    â”‚
  â”‚  Generation  â”‚                    â”‚  tool call:         â”‚
  â”‚  y           â”‚                    â”‚  {name, arguments}  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  RUNTIME EXECUTOR    â”‚
                                      â”‚  Parse â†’ Validate â†’ â”‚
                                      â”‚  Execute             â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  Tool Result r      â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  M_Î¸ resumes with   â”‚
                                      â”‚  context âˆ¥ r        â”‚
                                      â”‚  â†’ generates y      â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4.4 Parallel vs. Sequential Tool Calling

**Sequential Tool Calling:**

$$y = \mathcal{M}_\theta\Big(x,\; r_1 = \mathcal{T}_1(a_1),\; r_2 = \mathcal{T}_2(a_2; r_1),\; \dots\Big)$$

Each tool call depends on the result of the previous one (data dependency chain).

**Parallel Tool Calling:**

$$\{r_1, r_2, \dots, r_p\} = \texttt{PARALLEL\_EXEC}\Big(\mathcal{T}_1(a_1),\; \mathcal{T}_2(a_2),\; \dots,\; \mathcal{T}_p(a_p)\Big)$$

$$y = \mathcal{M}_\theta(x, r_1, r_2, \dots, r_p)$$

The model emits multiple independent tool calls in a single generation step when no inter-dependencies exist.

## 4.5 Training Paradigms for Tool Use

### 4.5.1 Supervised Fine-Tuning on Tool-Call Traces

Given demonstrations $\mathcal{D}_{\text{tool}} = \{(x_i, \text{trace}_i)\}$ where each trace contains interleaved text, tool calls, and results:

$$\mathcal{L}_{\text{SFT}} = -\sum_{i} \sum_{t} \log P_\theta(w_t^{(i)} \mid w_{<t}^{(i)})$$

where the loss is computed **only on model-generated tokens** (tool calls and final answers), not on tool results (which are treated as given context).

### 4.5.2 Reinforcement Learning from Tool Feedback

Define a reward:

$$r(x, y, \text{tool\_calls}) = \underbrace{r_{\text{correct}}(y, y^*)}_{\text{answer quality}} + \lambda_1 \underbrace{r_{\text{efficiency}}(\text{tool\_calls})}_{\text{minimize redundant calls}} + \lambda_2 \underbrace{r_{\text{format}}(\text{tool\_calls})}_{\text{valid JSON schema}}$$

Optimize via PPO or DPO:

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}\Big[\log \sigma\Big(\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\Big)\Big]$$

where $y_w$ is the preferred trajectory (correct tool usage) and $y_l$ is the dispreferred one.

## 4.6 Tool-Call Decision Boundary

The model must learn a decision boundary in the latent space that partitions queries into:

$$\Omega_{\text{direct}} = \{x : \text{Confidence}_\theta(x) \geq \gamma \text{ without tools}\}$$

$$\Omega_{\text{tool}} = \{x : \text{Confidence}_\theta(x) < \gamma \text{ OR } x \in \mathcal{C}_{\text{computation}}\}$$

where $\mathcal{C}_{\text{computation}}$ is the class of queries requiring external computation (math, API lookup, code execution) regardless of model confidence.

## 4.7 Pseudo-Algorithm: Tool-Calling LLM

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: TOOL-CALLING-LLM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    x                                â–· User query
    M_Î¸                              â–· Tool-calling LLM
    T = {Tâ‚, Tâ‚‚, ..., Tâ‚™}           â–· Available tool specifications
    max_iterations                   â–· Maximum tool-call rounds
    EXECUTOR(Â·)                      â–· Sandboxed tool execution runtime

OUTPUT:
    y                                â–· Final natural language response

PROCEDURE:
    1.  context â† [
            SYSTEM_PROMPT,
            TOOL_DEFINITIONS(T),      â–· Serialized JSON schemas
            USER_MESSAGE(x)
        ]
    2.  iteration â† 0

    3.  LOOP:
    4.      iteration â† iteration + 1
    5.      IF iteration > max_iterations THEN:
    6.          RETURN M_Î¸.generate(context âˆ¥ "Provide best answer with available information.")

    7.      response â† M_Î¸.generate(context, stop_sequences=[EOS, </tool_call>])

    8.      IF response CONTAINS tool_call(s) THEN:
    9.          tool_calls â† PARSE_TOOL_CALLS(response)
                    â–· Extract list of {name, arguments} objects

   10.          â”€â”€â”€â”€ Validation â”€â”€â”€â”€
   11.          FOR each tc âˆˆ tool_calls DO:
   12.              VALIDATE_SCHEMA(tc.arguments, T[tc.name].parameters)
                        â–· Type checking, required field verification
   13.              IF validation fails THEN:
   14.                  context â† context âˆ¥ ASSISTANT(response) âˆ¥ 
                                  ERROR_MESSAGE(tc, validation_error)
   15.                  CONTINUE to next LOOP iteration
   16.          END FOR

   17.          â”€â”€â”€â”€ Execution â”€â”€â”€â”€
   18.          IF all tool_calls are independent THEN:
   19.              results â† PARALLEL_EXECUTE(
                        [(tc.name, tc.arguments) for tc âˆˆ tool_calls],
                        EXECUTOR
                    )
   20.          ELSE:
   21.              results â† SEQUENTIAL_EXECUTE(tool_calls, EXECUTOR)
   22.          END IF

   23.          â”€â”€â”€â”€ Context Update â”€â”€â”€â”€
   24.          context â† context âˆ¥ ASSISTANT(response)
   25.          FOR each (tc, result) âˆˆ ZIP(tool_calls, results) DO:
   26.              context â† context âˆ¥ TOOL_RESULT(tc.id, result)
   27.          END FOR

   28.      ELSE:
                â–· No tool call detected â€” this is the final response
   29.          RETURN response.text
   30.      END IF
   31.  END LOOP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm: Tool Selection via Semantic Routing

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: TOOL-SEMANTIC-ROUTER
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    x                                â–· User query
    T = {(Táµ¢, descáµ¢, embedáµ¢)}       â–· Tools with pre-computed description embeddings
    f_q(Â·)                           â–· Query encoder
    Î¸_threshold                      â–· Minimum similarity for tool activation

OUTPUT:
    T_active âŠ† T                     â–· Subset of tools to present to LLM

PROCEDURE:
    1.  x_emb â† f_q(x) / â€–f_q(x)â€–â‚‚
    2.  scores â† {}
    3.  FOR each (Táµ¢, descáµ¢, embedáµ¢) âˆˆ T DO:
    4.      scores[Táµ¢] â† COSINE(x_emb, embedáµ¢)
    5.  END FOR
    6.  T_active â† {Táµ¢ : scores[Táµ¢] â‰¥ Î¸_threshold}
    7.  T_active â† TOP-k(T_active, by scores)
            â–· Limit to manageable set to avoid context window bloat
    8.  RETURN T_active
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

# 5. LLM Augmentation with Agents

## 5.1 Definition

An **LLM Agent** is an autonomous system $\mathcal{A} = (\mathcal{M}_\theta, \mathcal{P}, \mathcal{T}, \mathcal{E}, \mathcal{S})$ where:


![Image description](assets/Augmented_LLM_Evolution_page_3.png)

- $\mathcal{M}_\theta$ â€” the backbone language model serving as the **cognitive core**
- $\mathcal{P}$ â€” the **planning module** (decomposition, goal-setting, strategy selection)
- $\mathcal{T} = \{T_1, \dots, T_n\}$ â€” the **tool set** (APIs, retrievers, code executors)
- $\mathcal{E}$ â€” the **environment** (the external world the agent can observe and act upon)
- $\mathcal{S}$ â€” the **state/memory module** (working memory, episodic memory, persistent memory)

The agent operates as a **cognitive loop** mapping observations to actions:

$$a_t = \pi_\theta(o_{\leq t}, m_t, g) = \mathcal{M}_\theta\Big(\text{PROMPT}\big(o_{\leq t}, m_t, g, \mathcal{T}\big)\Big)$$

where:
- $o_{\leq t}$ = observation history up to step $t$
- $m_t$ = current memory state
- $g$ = goal derived from the original user query
- $a_t \in \mathcal{A}_{\text{space}} = \{\text{tool\_call}, \text{think}, \text{respond}, \text{delegate}\}$

## 5.2 Agent Architecture â€” The Cognitive Architecture


![Image description](assets/Augmented_LLM_Evolution_page_4.png)


![Image description](assets/Augmented_LLM_Evolution_page_1.png)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM AGENT ARCHITECTURE                        â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    PERCEPTION MODULE                          â”‚   â”‚
â”‚   â”‚   Observation oâ‚œ â† PARSE(Environment feedback, Tool results) â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚

![Image description](assets/Augmented_LLM_Evolution_page_5.png)


![Image description](assets/Augmented_LLM_Evolution_page_2.png)

â”‚   â”‚                    MEMORY MODULE  S                           â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚   â”‚  â”‚ Working  â”‚  â”‚  Episodic    â”‚  â”‚  Semantic / Long-term  â”‚ â”‚   â”‚
â”‚   â”‚  â”‚ Memory   â”‚  â”‚  Memory      â”‚  â”‚  Memory (Vector DB)    â”‚ â”‚   â”‚
â”‚   â”‚  â”‚ (context â”‚  â”‚  (past       â”‚  â”‚  (persistent facts,    â”‚ â”‚   â”‚
â”‚   â”‚  â”‚  window) â”‚  â”‚   traces)    â”‚  â”‚   learned patterns)    â”‚ â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚

![Image description](assets/Augmented_LLM_Evolution_page_6.png)

â”‚   â”‚                 REASONING / PLANNING MODULE  P                â”‚   â”‚
â”‚   â”‚                                                               â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚   â”‚  Task        â”‚  â”‚  Strategy    â”‚  â”‚  Self-Reflection â”‚  â”‚   â”‚
â”‚   â”‚   â”‚  Decompose   â”‚  â”‚  Selection   â”‚  â”‚  & Critique      â”‚  â”‚   â”‚
â”‚   â”‚   â”‚  (subgoals)  â”‚  â”‚  (which tool â”‚  â”‚  (verify, retry) â”‚  â”‚   â”‚
â”‚   â”‚   â”‚              â”‚  â”‚   / method)  â”‚  â”‚                  â”‚  â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    ACTION MODULE                              â”‚   â”‚
â”‚   â”‚   aâ‚œ âˆˆ {tool_call, think, respond, delegate, wait}          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                     â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚          â–¼                     â–¼                      â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚   Tool     â”‚     â”‚  Internal    â”‚      â”‚  Final Response  â”‚     â”‚
â”‚   â”‚  Execution â”‚     â”‚  Reasoning   â”‚      â”‚  to User         â”‚     â”‚
â”‚   â”‚  Tâ‚...Tâ‚™  â”‚     â”‚  (CoT step)  â”‚      â”‚                  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                                                            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€ result râ‚œ â”€â”€â”€â”€â”€ fed back to Perception â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5.3 Agent Reasoning Frameworks

### 5.3.1 ReAct (Reason + Act)

ReAct interleaves **reasoning traces** (chain-of-thought) with **actions** (tool calls):

$$\text{Trajectory}_{\text{ReAct}} = (t_1, a_1, o_1, t_2, a_2, o_2, \dots, t_n, a_{\text{finish}})$$

where:
- $t_i$ = **Thought**: natural language reasoning about what to do next
- $a_i$ = **Action**: structured tool invocation or final answer
- $o_i$ = **Observation**: result returned from the environment/tool

The key advantage is **interpretability** â€” the thought traces provide an auditable reasoning chain.

**Formal generation at each step:**

$$t_i, a_i \sim P_\theta\big(\cdot \mid q, t_1, a_1, o_1, \dots, t_{i-1}, a_{i-1}, o_{i-1}\big)$$

### 5.3.2 Plan-and-Execute

Separates **planning** from **execution** into distinct phases:

**Planning Phase:**

$$\mathcal{G} = \{g_1, g_2, \dots, g_m\} = \text{PLANNER}(\mathcal{M}_\theta, q)$$

where each $g_i$ is a subgoal with defined inputs, expected outputs, and dependencies.

**Execution Phase:**

$$r_i = \text{EXECUTOR}(\mathcal{M}_\theta, g_i, \{r_j : j \in \text{deps}(g_i)\}, \mathcal{T})$$

**Re-Planning:**

After each execution step, the planner can revise the remaining plan:

$$\mathcal{G}_{i+1:m}' = \text{REPLAN}(\mathcal{M}_\theta, q, r_{\leq i}, \mathcal{G}_{i+1:m})$$

### 5.3.3 Reflexion

Adds an explicit **self-evaluation** loop with persistent memory:

$$\text{Evaluation}_t = \mathcal{M}_\theta(\text{"Evaluate trajectory"}, \tau_t, q)$$

$$\text{Reflection}_t = \mathcal{M}_\theta(\text{"What went wrong and how to improve"}, \tau_t, \text{Eval}_t)$$

$$\mathcal{S}_{\text{memory}} \leftarrow \mathcal{S}_{\text{memory}} \cup \{\text{Reflection}_t\}$$

The next attempt conditions on accumulated reflections:

$$\tau_{t+1} \sim \pi_\theta(\cdot \mid q, \mathcal{S}_{\text{memory}})$$

### 5.3.4 Tree of Thoughts (ToT)

Explores multiple reasoning paths via tree search:

$$V(s) = \mathcal{M}_\theta(\text{"Evaluate state progress"}, s, q) \in [0, 1]$$

Search strategies include BFS and DFS with pruning:

$$s_{t+1}^{(j)} \sim P_\theta(\cdot \mid s_t^{(i)}) \quad \text{for } j \in \{1, \dots, b\}$$

where $b$ is the branching factor. Prune branches where $V(s) < \delta$.

## 5.4 Memory Systems in Agents

### 5.4.1 Memory Taxonomy

| Memory Type | Analogy | Implementation | Persistence |
|---|---|---|---|
| **Working Memory** | Human short-term memory | Context window of $\mathcal{M}_\theta$ | Per-session |
| **Episodic Memory** | Past experiences | Stored action-observation traces | Cross-session |
| **Semantic Memory** | General knowledge | Vector database of facts | Permanent |
| **Procedural Memory** | Skills / habits | Fine-tuned weights or prompt templates | Permanent |

### 5.4.2 Memory Compression

When the trajectory exceeds context window $L_{\max}$:

$$m_{\text{compressed}} = \mathcal{M}_\theta\big(\text{"Summarize key information"},\; \tau_{1:t}\big)$$

**Sliding-window with summary:**

$$\text{Context}_t = [m_{\text{compressed}(1:t-w)} \;\|\; \tau_{t-w:t}]$$

ensuring total length $\leq L_{\max}$.

## 5.5 Multi-Agent Systems

### 5.5.1 Definition

A **Multi-Agent System (MAS)** consists of $n$ agents $\{\mathcal{A}_1, \dots, \mathcal{A}_n\}$, each potentially with different roles, tools, and specialized prompts, interacting through a **communication protocol** $\Pi$:

$$\Pi: \mathcal{A}_i \xrightarrow{\text{message}} \mathcal{A}_j$$

### 5.5.2 Communication Topologies

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Star     â”‚        â”‚   Chain    â”‚        â”‚   Full     â”‚
    â”‚            â”‚        â”‚            â”‚        â”‚   Mesh     â”‚
    â”‚    Aâ‚      â”‚        â”‚ Aâ‚â†’Aâ‚‚â†’Aâ‚ƒ  â”‚        â”‚  Aâ‚â”€â”€Aâ‚‚   â”‚
    â”‚   /|\      â”‚        â”‚            â”‚        â”‚  |\ /|    â”‚
    â”‚  Aâ‚‚Aâ‚ƒAâ‚„   â”‚        â”‚            â”‚        â”‚  | X |    â”‚
    â”‚            â”‚        â”‚            â”‚        â”‚  |/ \|    â”‚
    â”‚ (Hub-Spoke)â”‚        â”‚ (Pipeline) â”‚        â”‚  Aâ‚ƒâ”€â”€Aâ‚„   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Hierarchy â”‚        â”‚  Debate    â”‚
    â”‚            â”‚        â”‚            â”‚
    â”‚   Manager  â”‚        â”‚  Aâ‚ âŸº Aâ‚‚  â”‚
    â”‚   / \      â”‚        â”‚    Judge   â”‚
    â”‚  Aâ‚  Aâ‚‚   â”‚        â”‚    Aâ‚ƒ      â”‚
    â”‚  /\  /\   â”‚        â”‚            â”‚
    â”‚ wâ‚wâ‚‚wâ‚ƒwâ‚„ â”‚        â”‚(Adversarialâ”‚
    â”‚            â”‚        â”‚ refinement)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.5.3 Multi-Agent Coordination Equation

For a team of agents solving task $x$:

$$y = \text{ORCHESTRATOR}\Big(\{r_i\}_{i=1}^{n}\Big) \quad \text{where} \quad r_i = \mathcal{A}_i\Big(x_i, \text{shared\_state}, \{m_{j \to i}\}_{j \neq i}\Big)$$

- $x_i$ = subtask assigned to agent $\mathcal{A}_i$
- $m_{j \to i}$ = messages received from other agents
- $\text{shared\_state}$ = globally visible workspace (e.g., shared document, codebase)

## 5.6 Pseudo-Algorithms

### Algorithm: ReAct Agent

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: REACT-AGENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· User objective
    M_Î¸                              â–· LLM backbone
    T = {Tâ‚, ..., Tâ‚™}               â–· Available tools
    max_steps                        â–· Maximum reasoning-action cycles
    EXECUTOR(Â·)                      â–· Tool execution sandbox

OUTPUT:
    y                                â–· Final answer
    Ï„                                â–· Full trajectory (for auditability)

PROCEDURE:
    1.  Ï„ â† []                       â–· Trajectory: list of (thought, action, observation)
    2.  context â† [
            SYSTEM_PROMPT_REACT,
            TOOL_DEFINITIONS(T),
            USER(q)
        ]

    3.  FOR step = 1 TO max_steps DO:

        â”€â”€â”€â”€ Thought Generation â”€â”€â”€â”€
    4.      thought â† M_Î¸.generate(
                context,
                prefix="Thought: ",
                stop=["Action:"]
            )
    5.      context â† context âˆ¥ "Thought: " âˆ¥ thought

        â”€â”€â”€â”€ Action Selection â”€â”€â”€â”€
    6.      action_str â† M_Î¸.generate(
                context,
                prefix="Action: ",
                stop=["Observation:"]
            )
    7.      (action_type, action_input) â† PARSE_ACTION(action_str)

    8.      IF action_type = "FINISH" THEN:
    9.          y â† action_input
   10.          Ï„.APPEND((thought, "FINISH", y))
   11.          RETURN (y, Ï„)
   12.      END IF

        â”€â”€â”€â”€ Execution â”€â”€â”€â”€
   13.      IF action_type âˆ‰ NAMES(T) THEN:
   14.          observation â† "Error: Unknown tool '" âˆ¥ action_type âˆ¥ 
                              "'. Available: " âˆ¥ NAMES(T)
   15.      ELSE:
   16.          observation â† EXECUTOR(T[action_type], action_input)
                    â–· Returns result string or error message
   17.      END IF

   18.      context â† context âˆ¥ "Action: " âˆ¥ action_str âˆ¥ 
                                "Observation: " âˆ¥ observation
   19.      Ï„.APPEND((thought, action_str, observation))

        â”€â”€â”€â”€ Memory Management â”€â”€â”€â”€
   20.      IF LENGTH(context) > 0.9 Â· L_max THEN:
   21.          summary â† M_Î¸.generate("Summarize key findings so far: " âˆ¥ Ï„)
   22.          context â† [SYSTEM_PROMPT_REACT, TOOL_DEFINITIONS(T),
                           "Previous progress summary: " âˆ¥ summary,
                           USER(q)]
   23.      END IF

   24.  END FOR

        â”€â”€â”€â”€ Forced Termination â”€â”€â”€â”€
   25.  y â† M_Î¸.generate(context âˆ¥ 
            "Maximum steps reached. Provide the best answer based on 
             information gathered so far.")
   26.  RETURN (y, Ï„)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm: Plan-and-Execute Agent

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: PLAN-AND-EXECUTE-AGENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· User objective
    M_Î¸_planner                      â–· Planning LLM (may be same as executor)
    M_Î¸_executor                     â–· Execution LLM
    T = {Tâ‚, ..., Tâ‚™}               â–· Available tools
    max_replans                      â–· Maximum re-planning cycles

OUTPUT:
    y                                â–· Final synthesized answer
    plan_trace                       â–· Executed plan with results

PROCEDURE:
    â”€â”€â”€â”€ PHASE 1: INITIAL PLANNING â”€â”€â”€â”€
    1.  plan â† M_Î¸_planner.generate(
            "Decompose this task into a numbered sequence of subtasks. 
             Each subtask should specify: description, required tool(s), 
             inputs, expected output, dependencies on prior steps."
            âˆ¥ q âˆ¥ TOOL_DEFINITIONS(T)
        )
    2.  steps â† PARSE_PLAN(plan)
            â–· steps = [{id, description, tool, depends_on, status}]
    3.  results â† {}
    4.  replan_count â† 0

    â”€â”€â”€â”€ PHASE 2: EXECUTION â”€â”€â”€â”€
    5.  WHILE âˆƒ step âˆˆ steps WITH step.status = "pending" DO:

    6.      executable â† {s âˆˆ steps : s.status = "pending" 
                          AND âˆ€ dep âˆˆ s.depends_on: results[dep] â‰  âŠ¥}
                â–· Find steps whose dependencies are satisfied

    7.      IF executable = âˆ… THEN:
    8.          â–· Deadlock or dependency failure
    9.          BREAK
   10.      END IF

   11.      FOR each step_s âˆˆ executable DO:
                â–· Can parallelize independent steps

   12.          dep_results â† {dep_id: results[dep_id] for dep_id âˆˆ step_s.depends_on}

   13.          execution_result â† M_Î¸_executor.generate(
                    "Execute this subtask:" âˆ¥ step_s.description âˆ¥
                    "Previous results:" âˆ¥ FORMAT(dep_results) âˆ¥
                    "Available tools:" âˆ¥ TOOL_DEFINITIONS(T) âˆ¥
                    "Original query:" âˆ¥ q
                )
                â–· The executor may invoke tools via TOOL-CALLING-LLM subroutine

   14.          results[step_s.id] â† execution_result
   15.          step_s.status â† "completed"
   16.      END FOR

        â”€â”€â”€â”€ PHASE 2.5: RE-PLANNING CHECK â”€â”€â”€â”€
   17.      progress_assessment â† M_Î¸_planner.generate(
                "Assess progress toward the goal. Are remaining steps 
                 still appropriate given results so far?" âˆ¥ 
                 FORMAT(steps, results) âˆ¥ q
            )

   18.      IF progress_assessment INDICATES need_to_replan THEN:
   19.          IF replan_count â‰¥ max_replans THEN:
   20.              â–· Force continuation with current plan
   21.              CONTINUE
   22.          END IF
   23.          revised_plan â† M_Î¸_planner.generate(
                    "Revise the remaining plan given current results." âˆ¥
                    FORMAT(completed_steps, results) âˆ¥ q
                )
   24.          steps â† MERGE(completed_steps, PARSE_PLAN(revised_plan))
   25.          replan_count â† replan_count + 1
   26.      END IF

   27.  END WHILE

    â”€â”€â”€â”€ PHASE 3: SYNTHESIS â”€â”€â”€â”€
   28.  y â† M_Î¸_planner.generate(
            "Synthesize a comprehensive final answer from all subtask results." âˆ¥
            FORMAT(steps, results) âˆ¥ q
        )

   29.  RETURN (y, {steps, results})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm: Reflexion Agent

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: REFLEXION-AGENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· User objective
    M_Î¸                              â–· LLM backbone
    EVALUATOR(Â·)                     â–· Evaluation function (heuristic, LLM-judge, or env reward)
    T                                â–· Available tools
    max_trials                       â–· Maximum retry attempts
    success_threshold                â–· Score above which to accept answer

OUTPUT:
    y_best                           â–· Best answer found
    reflections                      â–· Accumulated self-reflections

PROCEDURE:
    1.  long_term_memory â† []        â–· Persistent reflection storage
    2.  best_score â† -âˆ
    3.  y_best â† âŠ¥

    4.  FOR trial = 1 TO max_trials DO:

        â”€â”€â”€â”€ Generate Attempt â”€â”€â”€â”€
    5.      IF trial = 1 THEN:
    6.          Ï„ â† REACT_AGENT(q, M_Î¸, T)
                    â–· First attempt without reflections
    7.      ELSE:
    8.          context â† [
                    SYSTEM_PROMPT,
                    "Previous reflections on what to avoid and improve:",
                    FORMAT(long_term_memory),
                    USER(q)
                ]
    9.          Ï„ â† REACT_AGENT(q, M_Î¸, T, 
                    additional_context=FORMAT(long_term_memory))
   10.      END IF

   11.      y â† EXTRACT_ANSWER(Ï„)

        â”€â”€â”€â”€ Evaluate â”€â”€â”€â”€
   12.      score â† EVALUATOR(q, y, Ï„)
                â–· Score âˆˆ [0, 1], can be LLM-judge, unit test pass rate, 
                â–· environment reward, or human proxy

   13.      IF score > best_score THEN:
   14.          best_score â† score
   15.          y_best â† y
   16.      END IF

   17.      IF score â‰¥ success_threshold THEN:
   18.          RETURN (y_best, long_term_memory)
   19.      END IF

        â”€â”€â”€â”€ Self-Reflect â”€â”€â”€â”€
   20.      reflection â† M_Î¸.generate(
                "You attempted to solve this task and received score " âˆ¥ 
                score âˆ¥ ". Analyze what went wrong in your trajectory. 
                Identify specific errors, incorrect assumptions, and 
                what you should do differently next time." âˆ¥
                "Trajectory:" âˆ¥ FORMAT(Ï„) âˆ¥
                "Query:" âˆ¥ q
            )

   21.      long_term_memory.APPEND({
                trial: trial,
                score: score,
                reflection: reflection,
                key_errors: EXTRACT_ERRORS(reflection)
            })

   22.  END FOR

   23.  RETURN (y_best, long_term_memory)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Algorithm: Multi-Agent Orchestrator

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ALGORITHM: MULTI-AGENT-ORCHESTRATOR
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INPUT:
    q                                â–· Complex user objective
    Agents = {Aâ‚, ..., Aâ‚™}          â–· Specialized agents with roles
    M_orchestrator                   â–· Orchestrator LLM
    comm_topology                    â–· âˆˆ {star, chain, mesh, hierarchy}
    max_rounds                       â–· Maximum communication rounds

OUTPUT:
    y                                â–· Final consolidated answer
    transcript                       â–· Full inter-agent communication log

PROCEDURE:
    1.  shared_workspace â† {}        â–· Shared state visible to all agents
    2.  message_queue â† PriorityQueue()
    3.  transcript â† []

    â”€â”€â”€â”€ TASK DECOMPOSITION â”€â”€â”€â”€
    4.  assignments â† M_orchestrator.generate(
            "Decompose into subtasks and assign to agents based on expertise:" âˆ¥
            "Agents:" âˆ¥ FORMAT([{a.name, a.role, a.capabilities} for a âˆˆ Agents]) âˆ¥
            "Task:" âˆ¥ q
        )
    5.  task_map â† PARSE_ASSIGNMENTS(assignments)
            â–· task_map[Aáµ¢] = {subtask, depends_on, priority}

    â”€â”€â”€â”€ EXECUTION LOOP â”€â”€â”€â”€
    6.  FOR round = 1 TO max_rounds DO:

    7.      active_agents â† SELECT_READY_AGENTS(task_map, shared_workspace)

    8.      round_results â† {}
    9.      FOR each Aáµ¢ âˆˆ active_agents DO:     â–· Can parallelize

   10.          incoming_msgs â† GET_MESSAGES(message_queue, recipient=Aáµ¢)

   11.          agent_context â† [
                    Aáµ¢.system_prompt,       â–· Role-specific prompt
                    "Your assigned subtask:" âˆ¥ task_map[Aáµ¢].subtask,
                    "Messages from other agents:" âˆ¥ FORMAT(incoming_msgs),
                    "Shared workspace:" âˆ¥ FORMAT(shared_workspace),
                    "Original objective:" âˆ¥ q
                ]

   12.          (result, outgoing_msgs) â† Aáµ¢.execute(agent_context)
                    â–· Agent may use tools, reason, and produce messages for others

   13.          round_results[Aáµ¢] â† result
   14.          shared_workspace[Aáµ¢.name] â† result

   15.          FOR each msg âˆˆ outgoing_msgs DO:
   16.              message_queue.ENQUEUE(msg)
                        â–· msg = {sender, recipient, content, priority}
   17.          END FOR

   18.          transcript.APPEND({
                    round: round, agent: Aáµ¢.name, 
                    result: result, messages_sent: outgoing_msgs
                })
   19.      END FOR

        â”€â”€â”€â”€ CONVERGENCE CHECK â”€â”€â”€â”€
   20.      convergence â† M_orchestrator.generate(
                "Have all subtasks been completed satisfactorily? 
                 Is additional coordination needed?" âˆ¥
                FORMAT(shared_workspace) âˆ¥ q
            )

   21.      IF PARSE_DECISION(convergence) = "complete" THEN:
   22.          BREAK
   23.      ELSE IF PARSE_DECISION(convergence) = "reassign" THEN:
   24.          task_map â† M_orchestrator.REPLAN(task_map, shared_workspace)
   25.      END IF

   26.  END FOR

    â”€â”€â”€â”€ SYNTHESIS â”€â”€â”€â”€
   27.  y â† M_orchestrator.generate(
            "Synthesize all agent contributions into a final comprehensive answer:" âˆ¥
            FORMAT(shared_workspace) âˆ¥ q
        )

   28.  RETURN (y, transcript)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## 5.7 Agent Evaluation Metrics


![Image description](assets/Augmented_LLM_Evolution_page_14.png)

| Metric | Definition |
|---|---|
| **Task Success Rate** | $\frac{|\{q : \text{EVALUATOR}(q, y) \geq \theta\}|}{|\mathcal{B}|}$ |
| **Average Steps to Completion** | $\mathbb{E}[|\tau|]$ over successful trajectories |
| **Tool Call Accuracy** | $\frac{|\text{valid and necessary tool calls}|}{|\text{total tool calls}|}$ |
| **Reflection Improvement Rate** | $\frac{\text{score}_{\text{trial}_{t+1}} - \text{score}_{\text{trial}_t}}{\text{score}_{\text{trial}_t}}$ |
| **Cost Efficiency** | $\frac{\text{task\_success\_rate}}{\text{total\_tokens\_consumed}}$ |

---

# Summary: Unified View of Augmented LLMs


![Image description](assets/Augmented_LLM_Evolution_page_10.png)


![Image description](assets/Augmented_LLM_Evolution_page_9.png)


![Image description](assets/Augmented_LLM_Evolution_page_7.png)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚                    AUGMENTED LLM SPECTRUM                         â”‚
â”‚                                                                  â”‚
â”‚   Complexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º     â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  Naive  â”‚    â”‚Advanced â”‚    â”‚   Tool    â”‚    â”‚  Agentic  â”‚ â”‚
â”‚   â”‚  RAG    â”‚â”€â”€â”€â–¶â”‚  RAG    â”‚â”€â”€â”€â–¶â”‚  Calling  â”‚â”€â”€â”€â–¶â”‚  Systems  â”‚ â”‚

![Image description](assets/Augmented_LLM_Evolution_page_8.png)

â”‚   â”‚         â”‚    â”‚+Rerank  â”‚    â”‚  + RAG    â”‚    â”‚           â”‚ â”‚
â”‚   â”‚ Single  â”‚    â”‚+Rewrite â”‚    â”‚           â”‚    â”‚ Planning  â”‚ â”‚
â”‚   â”‚retrieve â”‚    â”‚+Self-RAGâ”‚    â”‚ Parallel/ â”‚    â”‚ Memory    â”‚ â”‚
â”‚   â”‚+generateâ”‚    â”‚+CRAG    â”‚    â”‚ Sequentialâ”‚    â”‚ Reflectionâ”‚ â”‚
â”‚   â”‚         â”‚    â”‚         â”‚    â”‚           â”‚    â”‚ Multi-Agntâ”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚   Autonomy:  Low          Medium         Medium-High      High  â”‚
â”‚   Latency:   Low          Medium         Medium           High  â”‚
â”‚   Accuracy:  Medium       High           High             V.Highâ”‚
â”‚   Cost:      Low          Medium         Medium           High  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


![Image description](assets/Augmented_LLM_Evolution_page_15.png)


![Image description](assets/Augmented_LLM_Evolution_page_12.png)


![Image description](assets/Augmented_LLM_Evolution_page_11.png)

The progression from RAG â†’ Tool Calling â†’ Agents represents an increase along three axes: **autonomy** (the system's ability to self-direct), **capability** (range of problems solvable), and **complexity** (engineering and evaluation difficulty). The optimal operating point depends on the task's requirements for accuracy, latency, cost, and the degree of trust placed in autonomous model behavior.
![Image description](assets/Augmented_LLM_Evolution_page_13.png)

